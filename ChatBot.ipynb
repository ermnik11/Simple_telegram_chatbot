{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import logging\n",
    "\n",
    "from telegram import Update\n",
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'sberbank-ai/rugpt3large_based_on_gpt2'\n",
    "#model_name = 'Grossmend/rudialogpt3_medium_based_on_gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_dialog(texts):\n",
    "    prefix = '\\nx:'\n",
    "    for i, t in enumerate(texts):\n",
    "        prefix += t\n",
    "        prefix += '\\nx:' if i % 2 == 1 else '\\ny:'\n",
    "    tokens = tokenizer(prefix, return_tensors='pt')\n",
    "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "    end_token_id = tokenizer.encode('\\n')[0]\n",
    "    size = tokens['input_ids'].shape[1]\n",
    "    output = model.generate(\n",
    "        **tokens, \n",
    "        eos_token_id=end_token_id,\n",
    "        do_sample=True, \n",
    "        max_length=size+128, \n",
    "        repetition_penalty=5.0, \n",
    "        temperature=1.1,\n",
    "        num_beams=5,\n",
    "        length_penalty=0.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    decoded = tokenizer.decode(output[0])\n",
    "    result = decoded[len(prefix):]\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few command handlers. These usually take the two arguments update and\n",
    "# context. Error handlers also receive the raised TelegramError object in error.\n",
    "history = []\n",
    "def start(update: Update, context: CallbackContext):\n",
    "    update.message.reply_text('Нейросеть не знает, что говорит, и может сказать всякое —\\n если что, не обижайтесь. Распространяя получившиеся\\n тексты, помните об ответственности.')\n",
    "\n",
    "def echo(update: Update, context: CallbackContext):\n",
    "    txt = update.message.text\n",
    "    \n",
    "    history.append(txt)\n",
    "    result = respond_to_dialog(history[-10:])\n",
    "    history.append(result)\n",
    "    \n",
    "    update.message.reply_text(result)\n",
    "    \n",
    "    with open(\"log.txt\", \"a\") as file:\n",
    "        print(str(update.message.chat_id) + \": \" + txt, file=file)\n",
    "        print(\"bot: \" + result, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikolay\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:2158: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    }
   ],
   "source": [
    "updater = Updater(\"put your telegram bot private token here\", use_context=True)\n",
    "dispatcher = updater.dispatcher\n",
    "\n",
    "# on different commands - answer in Telegram\n",
    "dispatcher.add_handler(CommandHandler(\"start\", start))\n",
    "dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, echo))\n",
    "\n",
    "# Start the Bot\n",
    "updater.start_polling()\n",
    "updater.idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
